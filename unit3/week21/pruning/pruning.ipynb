{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lucas Ross 21 Mar 2023\n",
    "\n",
    "#import libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Pruning\n",
    "- stop growing the tree before its too big\n",
    "- do this with hyperparameters in sklearn tree\n",
    "\n",
    "``max_depth`` max depth (height) of tree\n",
    "\n",
    "``min_samples_split`` min samples required for a split to be made\n",
    "\n",
    "``min_samples_leaf`` min samples required for a leaf to be made"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Pruning\n",
    "\n",
    "- build a complete tree then remove subtrees\n",
    "- calculate alpha, take pruned tree with lowest alpha (effect of pruning on regular tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "- finds best hyperparameters from dataset\n",
    "```\n",
    "criteria 1 = Gini, max_depth = 4\n",
    "criteria 2 = Entropy, max_depth = 7\n",
    "\n",
    "param_grid = {\"criterion\": [\"Gini\", \"Entropy\"], \"max_depth\":[4, 7]}\n",
    "```\n",
    "#### Method 1: run model with different criteria and use the highest accuracy\n",
    "- not the best :(\n",
    "#### Method 2: cross validation with grid search\n",
    "- create subsets of dataset, perform n iterations of model-fitting with each subset as the test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matix\n",
    "\n",
    "- gives comparison summary of predicted vs. actual results in classification\n",
    "```\n",
    "_________________________________\n",
    "|               |      |        |\n",
    "| actual 1 = 0  |  tp  |   fn   |\n",
    "|_______________|______|________|\n",
    "|               |      |        |\n",
    "| actual 2 = 1  |  fp  |  tn    |\n",
    "|_______________|______|________|\n",
    "```\n",
    "\n",
    "- positive(p): predicted result is positive (1)\n",
    "- negative(n): predicted result is negative (0)\n",
    "- true negative(tn): predicted and actual = 0\n",
    "- true positive(tp): predicted and actual = 1\n",
    "- false negative(fn): predicted = 0, actual = 1\n",
    "- false positive(fp): predicted = 1, actual = 0\n",
    "\n",
    "``accuracy = (tp + tn) / (tp + tn + fp + fn)``\n",
    "\n",
    "``sensitivity or true positive rate (tpr) = tp / (tp + fn)``\n",
    "\n",
    "``specificity or true negative rate (tnr) = tn / (tn + fp)``\n",
    "\n",
    "`` false positive rate = fp / (tp + fp)``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
